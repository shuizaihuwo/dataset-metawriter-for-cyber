# Usage Examples

This directory contains practical examples of using the Dataset Metadata Annotation Tool.

## Example Dataset Structure

```
datasets/
├── qiaoyu-20250414-CyberBench/
│   ├── README.md                    # Automatically parsed by tool
│   ├── data/
│   │   ├── train.json
│   │   ├── test.json
│   │   └── validation.json
│   ├── scripts/
│   │   └── evaluate.py
│   ├── meta.json                    # Generated by tool
│   └── meta.md                      # Generated by tool
└── qiaoyu-20250414-VulnDB/
    ├── dataset.md                   # Automatically parsed by tool
    ├── vulnerabilities.csv
    ├── meta.json                    # Generated by tool
    └── meta.md                      # Generated by tool
```

## Basic Usage Examples

### 1. Process Single Dataset

```bash
# Process a single dataset with automatic document parsing
python -m dsmeta.cli run /path/to/qiaoyu-20250414-CyberBench

# Expected output:
# ✓ Successfully processed dataset: CyberBench
# Generated files:
#   - meta.json (structured metadata)
#   - meta.md (human-readable documentation)
```

### 2. Batch Export to CSV

```bash
# Export all processed datasets to CSV
python -m dsmeta.cli export-csv /path/to/datasets --summary

# Expected output:
# 📊 开始导出数据集信息
# ✓ CSV导出完成!
# 📈 数据集统计摘要:
# │ 总数据集数 │ 15   │
# │ 平均文件数 │ 73.0 │
```

### 3. Configuration Examples

```bash
# Use custom configuration
python -m dsmeta.cli -c custom-config.yaml run /path/to/dataset

# Show current configuration
python -m dsmeta.cli config-show

# Initialize new configuration
python -m dsmeta.cli config-init
```

## Document Parsing Examples

The tool automatically identifies and parses documentation files in your datasets:

### Example README.md Structure
```markdown
# CyberBench Dataset

## 数据集描述
网络安全领域的综合基准测试数据集，包含多种网络安全相关的评测任务

## 数据集来源
https://github.com/example/CyberBench

## 数据集用途
模型评测

## 数据模态
代码

## 赋能业务方向
代码分析

## 赋能业务点
静态分析
```

### Intelligent Expansion Result
The tool will expand simple document information:

**Input from README:**
- 赋能业务方向: "代码分析" (single)
- 赋能业务点: "静态分析" (single)

**Output in metadata:**
- business_direction: ["代码分析", "漏洞挖掘", "策略规划"] (expanded)
- business_point: ["静态分析", "脆弱性分析", "代码辅助生成"] (expanded)

## Generated Metadata Examples

### Example meta.json
```json
{
  "id": "abc123def456",
  "name": "CyberBench",
  "description": "网络安全领域的综合基准测试数据集，包含多种网络安全相关的评测任务",
  "source": "GitHub",
  "source_url": "https://github.com/example/CyberBench",
  "size": "15.2MB",
  "num_files": 124,
  "num_records": 5000,
  "languages": ["zh", "en"],
  "modality": "代码",
  "use_case": "模型评测",
  "domain": "网络攻防",
  "business_direction": ["代码分析", "漏洞挖掘"],
  "business_point": ["静态分析", "脆弱性分析"],
  "rating": "高级",
  "file_formats": [
    {"format": ".py", "count": 45, "size": "8.1MB", "ratio": "53.3%"},
    {"format": ".json", "count": 29, "size": "2.1MB", "ratio": "13.8%"}
  ]
}
```

### Example meta.md
```markdown
# CyberBench

网络安全领域的综合基准测试数据集，包含多种网络安全相关的评测任务

## 基本信息

- **数据集ID**: `abc123def456`
- **数据集名称**: CyberBench
- **来源**: GitHub
- **官方链接**: https://github.com/example/CyberBench
- **总大小**: 15.2MB
- **文件数量**: 124
- **记录数量**: 5,000

## 技术属性

- **数据模态**: 代码
- **主要用途**: 模型评测
- **任务类型**: 代码分析, 漏洞检测, 安全评估

## 业务属性

- **专业领域**: 网络攻防
- **业务方向**: 代码分析, 漏洞挖掘
- **业务场景**: 静态分析, 脆弱性分析
- **专业评级**: 高级
```

## CSV Export Example

The exported CSV contains comprehensive metadata for all datasets:

| id | name | description | source | modality | domain | business_direction | business_point | ... |
|----|------|-------------|--------|----------|--------|-------------------|----------------|-----|
| abc123 | CyberBench | 网络安全基准测试... | GitHub | 代码 | 网络攻防 | 代码分析; 漏洞挖掘 | 静态分析; 脆弱性分析 | ... |
| def456 | VulnDB | 漏洞数据库... | Kaggle | 结构化/表格 | 网络攻防 | 情报分析 | 目标分析 | ... |

## Advanced Usage

### Custom Templates
```bash
# Create custom template
cp templates/default.md.j2 templates/cybersec.md.j2
# Edit cybersec.md.j2 for cybersecurity-specific format

# Use custom template
python -m dsmeta.cli run /path/to/dataset --template cybersec.md.j2
```

### Watch Mode
```bash
# Continuously monitor directory for new datasets
python -m dsmeta.cli watch /path/to/datasets
# Will automatically process any new dataset directories added
```

### Validation
```bash
# Validate existing metadata
python -m dsmeta.cli validate /path/to/dataset
# Checks metadata completeness and consistency
```

## Configuration Examples

### Minimal Configuration
```yaml
llm:
  api_key: "your-api-key-here"
  model: "THUDM/GLM-4-9B-0414"

output:
  output_formats: ["markdown", "json"]
```

### Advanced Configuration
```yaml
llm:
  provider: "siliconflow"
  model: "THUDM/GLM-4-9B-0414" 
  base_url: "https://api.siliconflow.cn/v1"
  api_key: "${SILICONFLOW_API_KEY}"
  temperature: 0.1
  max_tokens: 4000

file_processing:
  max_file_size: "100MB"
  sample_head_lines: 2000
  encoding_fallback: ["utf-8", "gbk", "latin-1"]

search:
  enabled: true
  provider: "tavily"
  api_key: "${TAVILY_API_KEY}"

quality_control:
  min_confidence_score: 0.8
  enum_validation: true
  required_fields: ["name", "description", "modality", "domain"]

output:
  template_dir: "./custom_templates"
  output_formats: ["markdown", "json", "yaml"]
  backup_existing: true
```

## Troubleshooting Examples

### Common Issues and Solutions

1. **"No artifacts to write" error**
   ```bash
   # Ensure all pipeline nodes complete successfully
   # Check logs for validation errors
   python -m dsmeta.cli run /path/to/dataset --force
   ```

2. **Invalid enum values**
   ```bash
   # Tool automatically fixes common enum mismatches
   # Check quality_control.enum_validation in config
   ```

3. **Missing required fields**
   ```bash
   # Tool automatically supplements missing fields
   # Customize required_fields in config if needed
   ```