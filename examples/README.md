# Usage Examples

This directory contains practical examples of using the Dataset Metadata Annotation Tool.

## Example Dataset Structure

```
datasets/
â”œâ”€â”€ qiaoyu-20250414-CyberBench/
â”‚   â”œâ”€â”€ README.md                    # Automatically parsed by tool
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â”œâ”€â”€ test.json
â”‚   â”‚   â””â”€â”€ validation.json
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ evaluate.py
â”‚   â”œâ”€â”€ meta.json                    # Generated by tool
â”‚   â””â”€â”€ meta.md                      # Generated by tool
â””â”€â”€ qiaoyu-20250414-VulnDB/
    â”œâ”€â”€ dataset.md                   # Automatically parsed by tool
    â”œâ”€â”€ vulnerabilities.csv
    â”œâ”€â”€ meta.json                    # Generated by tool
    â””â”€â”€ meta.md                      # Generated by tool
```

## Basic Usage Examples

### 1. Process Single Dataset

```bash
# Process a single dataset with automatic document parsing
python -m dsmeta.cli run /path/to/qiaoyu-20250414-CyberBench

# Expected output:
# âœ“ Successfully processed dataset: CyberBench
# Generated files:
#   - meta.json (structured metadata)
#   - meta.md (human-readable documentation)
```

### 2. Batch Export to CSV

```bash
# Export all processed datasets to CSV
python -m dsmeta.cli export-csv /path/to/datasets --summary

# Expected output:
# ğŸ“Š å¼€å§‹å¯¼å‡ºæ•°æ®é›†ä¿¡æ¯
# âœ“ CSVå¯¼å‡ºå®Œæˆ!
# ğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡æ‘˜è¦:
# â”‚ æ€»æ•°æ®é›†æ•° â”‚ 15   â”‚
# â”‚ å¹³å‡æ–‡ä»¶æ•° â”‚ 73.0 â”‚
```

### 3. Configuration Examples

```bash
# Use custom configuration
python -m dsmeta.cli -c custom-config.yaml run /path/to/dataset

# Show current configuration
python -m dsmeta.cli config-show

# Initialize new configuration
python -m dsmeta.cli config-init
```

## Document Parsing Examples

The tool automatically identifies and parses documentation files in your datasets:

### Example README.md Structure
```markdown
# CyberBench Dataset

## æ•°æ®é›†æè¿°
ç½‘ç»œå®‰å…¨é¢†åŸŸçš„ç»¼åˆåŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§ç½‘ç»œå®‰å…¨ç›¸å…³çš„è¯„æµ‹ä»»åŠ¡

## æ•°æ®é›†æ¥æº
https://github.com/example/CyberBench

## æ•°æ®é›†ç”¨é€”
æ¨¡å‹è¯„æµ‹

## æ•°æ®æ¨¡æ€
ä»£ç 

## èµ‹èƒ½ä¸šåŠ¡æ–¹å‘
ä»£ç åˆ†æ

## èµ‹èƒ½ä¸šåŠ¡ç‚¹
é™æ€åˆ†æ
```

### Intelligent Expansion Result
The tool will expand simple document information:

**Input from README:**
- èµ‹èƒ½ä¸šåŠ¡æ–¹å‘: "ä»£ç åˆ†æ" (single)
- èµ‹èƒ½ä¸šåŠ¡ç‚¹: "é™æ€åˆ†æ" (single)

**Output in metadata:**
- business_direction: ["ä»£ç åˆ†æ", "æ¼æ´æŒ–æ˜", "ç­–ç•¥è§„åˆ’"] (expanded)
- business_point: ["é™æ€åˆ†æ", "è„†å¼±æ€§åˆ†æ", "ä»£ç è¾…åŠ©ç”Ÿæˆ"] (expanded)

## Generated Metadata Examples

### Example meta.json
```json
{
  "id": "abc123def456",
  "name": "CyberBench",
  "description": "ç½‘ç»œå®‰å…¨é¢†åŸŸçš„ç»¼åˆåŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§ç½‘ç»œå®‰å…¨ç›¸å…³çš„è¯„æµ‹ä»»åŠ¡",
  "source": "GitHub",
  "source_url": "https://github.com/example/CyberBench",
  "size": "15.2MB",
  "num_files": 124,
  "num_records": 5000,
  "languages": ["zh", "en"],
  "modality": "ä»£ç ",
  "use_case": "æ¨¡å‹è¯„æµ‹",
  "domain": "ç½‘ç»œæ”»é˜²",
  "business_direction": ["ä»£ç åˆ†æ", "æ¼æ´æŒ–æ˜"],
  "business_point": ["é™æ€åˆ†æ", "è„†å¼±æ€§åˆ†æ"],
  "rating": "é«˜çº§",
  "file_formats": [
    {"format": ".py", "count": 45, "size": "8.1MB", "ratio": "53.3%"},
    {"format": ".json", "count": 29, "size": "2.1MB", "ratio": "13.8%"}
  ]
}
```

### Example meta.md
```markdown
# CyberBench

ç½‘ç»œå®‰å…¨é¢†åŸŸçš„ç»¼åˆåŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å«å¤šç§ç½‘ç»œå®‰å…¨ç›¸å…³çš„è¯„æµ‹ä»»åŠ¡

## åŸºæœ¬ä¿¡æ¯

- **æ•°æ®é›†ID**: `abc123def456`
- **æ•°æ®é›†åç§°**: CyberBench
- **æ¥æº**: GitHub
- **å®˜æ–¹é“¾æ¥**: https://github.com/example/CyberBench
- **æ€»å¤§å°**: 15.2MB
- **æ–‡ä»¶æ•°é‡**: 124
- **è®°å½•æ•°é‡**: 5,000

## æŠ€æœ¯å±æ€§

- **æ•°æ®æ¨¡æ€**: ä»£ç 
- **ä¸»è¦ç”¨é€”**: æ¨¡å‹è¯„æµ‹
- **ä»»åŠ¡ç±»å‹**: ä»£ç åˆ†æ, æ¼æ´æ£€æµ‹, å®‰å…¨è¯„ä¼°

## ä¸šåŠ¡å±æ€§

- **ä¸“ä¸šé¢†åŸŸ**: ç½‘ç»œæ”»é˜²
- **ä¸šåŠ¡æ–¹å‘**: ä»£ç åˆ†æ, æ¼æ´æŒ–æ˜
- **ä¸šåŠ¡åœºæ™¯**: é™æ€åˆ†æ, è„†å¼±æ€§åˆ†æ
- **ä¸“ä¸šè¯„çº§**: é«˜çº§
```

## CSV Export Example

The exported CSV contains comprehensive metadata for all datasets:

| id | name | description | source | modality | domain | business_direction | business_point | ... |
|----|------|-------------|--------|----------|--------|-------------------|----------------|-----|
| abc123 | CyberBench | ç½‘ç»œå®‰å…¨åŸºå‡†æµ‹è¯•... | GitHub | ä»£ç  | ç½‘ç»œæ”»é˜² | ä»£ç åˆ†æ; æ¼æ´æŒ–æ˜ | é™æ€åˆ†æ; è„†å¼±æ€§åˆ†æ | ... |
| def456 | VulnDB | æ¼æ´æ•°æ®åº“... | Kaggle | ç»“æ„åŒ–/è¡¨æ ¼ | ç½‘ç»œæ”»é˜² | æƒ…æŠ¥åˆ†æ | ç›®æ ‡åˆ†æ | ... |

## Advanced Usage

### Custom Templates
```bash
# Create custom template
cp templates/default.md.j2 templates/cybersec.md.j2
# Edit cybersec.md.j2 for cybersecurity-specific format

# Use custom template
python -m dsmeta.cli run /path/to/dataset --template cybersec.md.j2
```

### Watch Mode
```bash
# Continuously monitor directory for new datasets
python -m dsmeta.cli watch /path/to/datasets
# Will automatically process any new dataset directories added
```

### Validation
```bash
# Validate existing metadata
python -m dsmeta.cli validate /path/to/dataset
# Checks metadata completeness and consistency
```

## Configuration Examples

### Minimal Configuration
```yaml
llm:
  api_key: "your-api-key-here"
  model: "THUDM/GLM-4-9B-0414"

output:
  output_formats: ["markdown", "json"]
```

### Advanced Configuration
```yaml
llm:
  provider: "siliconflow"
  model: "THUDM/GLM-4-9B-0414" 
  base_url: "https://api.siliconflow.cn/v1"
  api_key: "${SILICONFLOW_API_KEY}"
  temperature: 0.1
  max_tokens: 4000

file_processing:
  max_file_size: "100MB"
  sample_head_lines: 2000
  encoding_fallback: ["utf-8", "gbk", "latin-1"]

search:
  enabled: true
  provider: "tavily"
  api_key: "${TAVILY_API_KEY}"

quality_control:
  min_confidence_score: 0.8
  enum_validation: true
  required_fields: ["name", "description", "modality", "domain"]

output:
  template_dir: "./custom_templates"
  output_formats: ["markdown", "json", "yaml"]
  backup_existing: true
```

## Troubleshooting Examples

### Common Issues and Solutions

1. **"No artifacts to write" error**
   ```bash
   # Ensure all pipeline nodes complete successfully
   # Check logs for validation errors
   python -m dsmeta.cli run /path/to/dataset --force
   ```

2. **Invalid enum values**
   ```bash
   # Tool automatically fixes common enum mismatches
   # Check quality_control.enum_validation in config
   ```

3. **Missing required fields**
   ```bash
   # Tool automatically supplements missing fields
   # Customize required_fields in config if needed
   ```